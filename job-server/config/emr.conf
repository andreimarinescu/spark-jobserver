spark {
 # spark.master will be passed to each job's JobContext
master = "yarn-client"
jobserver {
 port = 8090
 # Note: JobFileDAO is deprecated from v0.7.0 because of issues in
 # production and will be removed in future, now defaults to H2 file.
 jobdao = spark.jobserver.io.JobSqlDAO

 filedao {
   rootdir = /mnt/tmp/spark-jobserver/filedao/data
 }
 sqldao {
   # Slick database driver, full classpath
   slick-driver = slick.driver.H2Driver

   # JDBC driver, full classpath
   jdbc-driver = org.h2.Driver

   # Directory where default H2 driver stores its data. Only needed for H2.
   rootdir = /tmp/spark-jobserver/sqldao/data

   # Full JDBC URL / init string, along with username and password.  Sorry, needs to match above.
   # Substitutions may be used to launch job-server, but leave it out here in the default or tests won't pass
   jdbc {
     url = "jdbc:h2:file:/tmp/spark-jobserver/sqldao/data/h2-db"
     user = ""
     password = ""
   }

   # DB connection pool settings
   dbcp {
     enabled = false
     maxactive = 20
     maxidle = 10
     initialsize = 10
   }
 }
}
# predefined Spark contexts
contexts {
 # test {
 #   num-cpu-cores = 1            # Number of cores to allocate.  Required.
 #   memory-per-node = 1g         # Executor memory per node, -Xmx style eg 512m, 1G, etc.
 #   spark.executor.instances = 1
 # }
 # define additional contexts here
}
# universal context configuration.  These settings can be overridden, see README.md
context-settings {
 num-cpu-cores = 8          # Number of cores to allocate.  Required.
 memory-per-node = 8g         # Executor memory per node, -Xmx style eg 512m, #1G, etc.
 spark.executor.instances = 2
 # If you wish to pass any settings directly to the sparkConf as-is, add them here in passthrough,
 # such as hadoop connection settings that don't use the "spark." prefix
 passthrough {
   #es.nodes = "192.1.1.1"
 }
}
# This needs to match SPARK_HOME for cluster SparkContexts to be created successfully
home = "/usr/lib/spark"
}